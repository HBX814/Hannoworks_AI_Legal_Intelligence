{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12717461,"sourceType":"datasetVersion","datasetId":8037969}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q langchain langchain-community langchain-groq sentence-transformers faiss-cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T20:37:29.240370Z","iopub.execute_input":"2025-08-10T20:37:29.240644Z","iopub.status.idle":"2025-08-10T20:38:55.055526Z","shell.execute_reply.started":"2025-08-10T20:37:29.240619Z","shell.execute_reply":"2025-08-10T20:38:55.054795Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.5/443.5 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Dict, Any, Optional\nimport faiss\nfrom sentence_transformers import SentenceTransformer\nimport re\nfrom groq import Groq\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime\nimport pickle\nimport gradio as gr\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Configuration\n@dataclass\nclass Config:\n    GROQ_API_KEY: str = \"your_groq_api_key_here\"  # Replace with your actual API key\n    EMBEDDING_MODEL: str = \"all-MiniLM-L6-v2\"\n    LLM_MODEL: str = \"llama3-8b-8192\"  # Updated to working model\n    VECTOR_DIM: int = 384\n    CHUNK_SIZE: int = 1000\n    CHUNK_OVERLAP: int = 200\n    DATA_PATH: str = \"/kaggle/input/\"\n\nclass DocumentProcessor:\n    \"\"\"Handles document preprocessing and chunking\"\"\"\n    \n    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n    \n    def load_documents(self, file_path: str) -> str:\n        \"\"\"Load documents from text file\"\"\"\n        try:\n            with open(file_path, 'r', encoding='utf-8') as file:\n                content = file.read()\n            logger.info(f\"Loaded document with {len(content)} characters\")\n            return content\n        except Exception as e:\n            logger.error(f\"Error loading document: {e}\")\n            return \"\"\n    \n    def clean_text(self, text: str) -> str:\n        \"\"\"Clean and normalize text\"\"\"\n        text = re.sub(r'\\s+', ' ', text)\n        text = re.sub(r'\\n+', '\\n', text)\n        return text.strip()\n    \n    def chunk_text(self, text: str) -> List[Dict[str, Any]]:\n        \"\"\"Split text into overlapping chunks with metadata\"\"\"\n        chunks = []\n        words = text.split()\n        \n        for i in range(0, len(words), self.chunk_size - self.chunk_overlap):\n            chunk_words = words[i:i + self.chunk_size]\n            chunk_text = ' '.join(chunk_words)\n            \n            chunks.append({\n                'text': chunk_text,\n                'chunk_id': len(chunks),\n                'start_word': i,\n                'end_word': min(i + self.chunk_size, len(words)),\n                'word_count': len(chunk_words)\n            })\n        \n        logger.info(f\"Created {len(chunks)} chunks\")\n        return chunks\n\nclass VectorStore:\n    \"\"\"Handles vector embeddings and similarity search using FAISS\"\"\"\n    \n    def __init__(self, embedding_model: str = \"all-MiniLM-L6-v2\", vector_dim: int = 384):\n        self.model = SentenceTransformer(embedding_model)\n        self.vector_dim = vector_dim\n        # Use L2 distance instead of inner product for more stable results\n        self.index = faiss.IndexFlatL2(vector_dim)\n        self.documents = []\n        self.embeddings = []\n    \n    def add_documents(self, chunks: List[Dict[str, Any]]):\n        \"\"\"Add documents to vector store\"\"\"\n        texts = [chunk['text'] for chunk in chunks]\n        embeddings = self.model.encode(texts, normalize_embeddings=True)\n        \n        # Add to FAISS index\n        self.index.add(embeddings.astype('float32'))\n        \n        # Store documents and embeddings\n        self.documents.extend(chunks)\n        self.embeddings.extend(embeddings)\n        \n        logger.info(f\"Added {len(chunks)} documents to vector store\")\n    \n    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:\n        \"\"\"Search for similar documents\"\"\"\n        if len(self.documents) == 0:\n            return []\n            \n        query_embedding = self.model.encode([query], normalize_embeddings=True)\n        \n        # Search in FAISS index - using L2 distance (lower is better)\n        distances, indices = self.index.search(query_embedding.astype('float32'), min(k, len(self.documents)))\n        \n        results = []\n        for distance, idx in zip(distances[0], indices[0]):\n            if idx >= 0 and idx < len(self.documents):  # Valid index check\n                result = self.documents[idx].copy()\n                # Convert L2 distance to similarity score (higher is better)\n                similarity_score = 1.0 / (1.0 + float(distance))\n                result['similarity_score'] = similarity_score\n                results.append(result)\n        \n        # Sort by similarity score (highest first)\n        results.sort(key=lambda x: x['similarity_score'], reverse=True)\n        return results\n\nclass LegalReasoner:\n    \"\"\"Handles legal reasoning using LLM\"\"\"\n    \n    def __init__(self, api_key: str, model: str = \"llama3-8b-8192\"):\n        self.client = Groq(api_key=api_key)\n        self.model = model\n    \n    def analyze_case(self, case_description: str, relevant_precedents: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Analyze legal case with precedents\"\"\"\n        \n        precedent_context = \"\\n\\n\".join([\n            f\"Precedent {i+1} (Relevance: {doc['similarity_score']:.3f}):\\n{doc['text'][:500]}\"\n            for i, doc in enumerate(relevant_precedents[:3])  # Use top 3 to avoid token limits\n        ])\n        \n        system_prompt = \"\"\"You are an expert legal AI assistant specializing in Indian constitutional law and privacy rights. \n        Analyze legal cases by examining precedents and applying legal reasoning principles. \n        Provide balanced analysis for both sides of the argument. Keep responses concise and focused.\"\"\"\n        \n        user_prompt = f\"\"\"\n        Case Analysis Request:\n        \n        Current Case: {case_description[:1000]}\n        \n        Relevant Legal Precedents:\n        {precedent_context}\n        \n        Please provide a comprehensive legal analysis including:\n        1. Case Summary (2-3 sentences)\n        2. Key Legal Issues (3-4 main issues)\n        3. Applicable Precedents and their relevance\n        4. Arguments for the Petitioner (4-5 key points)\n        5. Arguments for the Government/Respondent (4-5 key points)\n        6. Likely Verdict and Reasoning (brief analysis)\n        7. Legal Implications (2-3 key implications)\n        \n        Format your response clearly with headings.\n        \"\"\"\n        \n        try:\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=[\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": user_prompt}\n                ],\n                max_tokens=3000,\n                temperature=0.3\n            )\n            \n            return {\n                'analysis': response.choices[0].message.content,\n                'model_used': self.model,\n                'timestamp': datetime.now().isoformat()\n            }\n        \n        except Exception as e:\n            logger.error(f\"Error in legal analysis: {e}\")\n            return {\n                'analysis': f'Error in analysis: {str(e)}\\n\\nPlease check your API key or try again.',\n                'model_used': self.model,\n                'timestamp': datetime.now().isoformat()\n            }\n    \n    def generate_arguments(self, case_description: str, side: str, precedents: List[Dict[str, Any]]) -> str:\n        \"\"\"Generate specific arguments for petitioner or respondent\"\"\"\n        \n        precedent_context = \"\\n\\n\".join([\n            f\"Precedent {i+1}:\\n{doc['text'][:400]}\"\n            for i, doc in enumerate(precedents[:2])  # Use top 2 to avoid token limits\n        ])\n        \n        system_prompt = f\"\"\"You are a legal expert preparing arguments for the {side} in an Indian constitutional law case. \n        Use relevant precedents and legal principles to build a strong argument. Be concise and focused.\"\"\"\n        \n        user_prompt = f\"\"\"\n        Case: {case_description[:800]}\n        \n        Relevant Precedents:\n        {precedent_context}\n        \n        Prepare detailed legal arguments for the {side}. Include:\n        1. Legal grounds and constitutional provisions (2-3 main grounds)\n        2. Precedent citations and their application (2-3 cases)\n        3. Policy considerations (2-3 points)\n        4. Potential counterarguments and responses (2-3 responses)\n        \n        Be specific and cite relevant case law and constitutional articles.\n        \"\"\"\n        \n        try:\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=[\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": user_prompt}\n                ],\n                max_tokens=1500,\n                temperature=0.4\n            )\n            \n            return response.choices[0].message.content\n            \n        except Exception as e:\n            logger.error(f\"Error generating arguments: {e}\")\n            return f\"Error generating arguments: {str(e)}\\n\\nPlease check your API key or try again.\"\n\nclass LegalAgent:\n    \"\"\"Main agentic system orchestrating the legal analysis workflow\"\"\"\n    \n    def __init__(self, config: Config):\n        self.config = config\n        self.document_processor = DocumentProcessor(config.CHUNK_SIZE, config.CHUNK_OVERLAP)\n        self.vector_store = VectorStore(config.EMBEDDING_MODEL, config.VECTOR_DIM)\n        self.legal_reasoner = LegalReasoner(config.GROQ_API_KEY, config.LLM_MODEL)\n        self.knowledge_base_ready = False\n    \n    def build_knowledge_base(self, file_path: str = None, text_content: str = None):\n        \"\"\"Build the legal knowledge base from documents or text\"\"\"\n        logger.info(\"Building knowledge base...\")\n        \n        if text_content and text_content.strip():\n            raw_text = text_content\n            logger.info(f\"Using provided text content ({len(text_content)} characters)\")\n        elif file_path and os.path.exists(file_path):\n            raw_text = self.document_processor.load_documents(file_path)\n            logger.info(f\"Loaded from file: {file_path}\")\n        else:\n            # Default sample legal text focused on Puttaswamy principles\n            raw_text = \"\"\"The Constitution of India, Article 21 states that \"No person shall be deprived of his life or personal liberty except according to procedure established by law.\" The Supreme Court in K.S. Puttaswamy v. Union of India (2017) unanimously declared that the right to privacy is a fundamental right under the Indian Constitution. The Court held that privacy is not an absolute right and can be restricted by the State, but any such restriction must satisfy the three-fold test: (i) legality - there must be a law; (ii) legitimate state aim - the law must seek to achieve a legitimate state aim; and (iii) proportionality - there must be a rational nexus between the objects and the means adopted. The Court further held that privacy includes at its core the preservation of personal intimacies, the sanctity of family life, marriage, procreation, the home and sexual orientation. Privacy also connotes a right to be left alone. The security of one's privacy against arbitrary intrusion by the police is basic to a free society. The right to privacy is protected as an intrinsic part of the right to life and personal liberty under Article 21 and as a part of the freedoms guaranteed by Part III of the Constitution. Privacy includes at its core the preservation of personal intimacies, the sanctity of family life, marriage, procreation, the home and sexual orientation. Privacy also connotes a right to be left alone. Privacy safeguards individual autonomy and recognises the ability of the individual to control vital aspects of his or her life. Personal autonomy includes the ability of an individual to make choices governing his or her life. Informational self-determination is a facet of the right to privacy. The development of technology and the growth of the means of mass communication along with the legitimate expectation that information which an individual has shared in confidence will not be shared with others, mandates recognition of the fundamental right to privacy. The Fundamental Rights guaranteed under Part III of the Constitution are natural rights inherent in the status of a citizen. Aadhaar enables the creation of a profile of an individual and serves the basis of a surveillance state. The biometric and demographic information of the individual is shared with requesting entities. There is a likelihood of commercial exploitation of biometric and demographic information by the private entities. The law must ensure the presence of safeguards against such misuse.\"\"\"\n            logger.info(\"Using default Puttaswamy-based legal text\")\n        \n        if not raw_text:\n            return False, \"Failed to load documents - no content available\"\n        \n        # Clean and chunk the text\n        clean_text = self.document_processor.clean_text(raw_text)\n        chunks = self.document_processor.chunk_text(clean_text)\n        \n        if not chunks:\n            return False, \"Failed to create text chunks\"\n        \n        # Add to vector store\n        try:\n            self.vector_store.add_documents(chunks)\n            self.knowledge_base_ready = True\n            logger.info(\"Knowledge base built successfully\")\n            return True, f\"Knowledge base built successfully with {len(chunks)} chunks\"\n        except Exception as e:\n            logger.error(f\"Error adding documents to vector store: {e}\")\n            return False, f\"Error building knowledge base: {str(e)}\"\n    \n    def query_case(self, case_description: str, num_precedents: int = 5) -> Dict[str, Any]:\n        \"\"\"Main method to analyze a legal case\"\"\"\n        if not self.knowledge_base_ready:\n            return {\"error\": \"Knowledge base not ready. Please build it first.\"}\n        \n        logger.info(f\"Processing case query: {case_description[:100]}...\")\n        \n        # Retrieve relevant precedents\n        relevant_precedents = self.vector_store.search(case_description, k=num_precedents)\n        \n        # Perform legal analysis\n        analysis = self.legal_reasoner.analyze_case(case_description, relevant_precedents)\n        \n        # Generate arguments for both sides\n        petitioner_args = self.legal_reasoner.generate_arguments(\n            case_description, \"Petitioner\", relevant_precedents\n        )\n        \n        respondent_args = self.legal_reasoner.generate_arguments(\n            case_description, \"Government/Respondent\", relevant_precedents\n        )\n        \n        result = {\n            'case_description': case_description,\n            'relevant_precedents': relevant_precedents,\n            'legal_analysis': analysis,\n            'petitioner_arguments': petitioner_args,\n            'respondent_arguments': respondent_args,\n            'processing_timestamp': datetime.now().isoformat()\n        }\n        \n        return result\n\n# Initialize global agent\nconfig = Config()\nagent = LegalAgent(config)\n\n# Gradio Interface Functions\ndef setup_knowledge_base(api_key, legal_text=None):\n    \"\"\"Setup the knowledge base\"\"\"\n    if not api_key.strip():\n        return \"❌ Please provide a valid Groq API key\", \"\"\n    \n    # Update the API key\n    config.GROQ_API_KEY = api_key\n    agent.legal_reasoner = LegalReasoner(api_key, config.LLM_MODEL)\n    \n    # Hardcoded path to the Puttaswamy judgment file\n    puttaswamy_file_path = \"/kaggle/input/datasethannoworks/puttaswamy_judgment.txt\"\n    \n    # Build knowledge base\n    if legal_text and legal_text.strip():\n        # Use provided text\n        success, message = agent.build_knowledge_base(text_content=legal_text)\n        source_info = \"Custom legal text provided\"\n    elif os.path.exists(puttaswamy_file_path):\n        # Use the hardcoded Puttaswamy file\n        success, message = agent.build_knowledge_base(file_path=puttaswamy_file_path)\n        source_info = f\"Loaded from: {puttaswamy_file_path}\"\n    else:\n        # Use default legal text if file not found\n        success, message = agent.build_knowledge_base()\n        source_info = f\"Default privacy law precedents (File not found at: {puttaswamy_file_path})\"\n    \n    if success:\n        status = f\"✅ {message}\"\n        kb_info = f\"Knowledge Base Status: Ready\\nChunks: {len(agent.vector_store.documents)}\\nEmbedding Model: {config.EMBEDDING_MODEL}\\nSource: {source_info}\"\n    else:\n        status = f\"❌ {message}\"\n        kb_info = \"Knowledge Base Status: Not Ready\"\n    \n    return status, kb_info\n\ndef analyze_legal_case(case_description, num_precedents):\n    \"\"\"Analyze a legal case\"\"\"\n    if not agent.knowledge_base_ready:\n        return \"❌ Knowledge base not ready. Please set up the knowledge base first.\", \"\", \"\", \"\", \"\"\n    \n    if not case_description.strip():\n        return \"❌ Please provide a case description.\", \"\", \"\", \"\", \"\"\n    \n    try:\n        # Analyze the case\n        result = agent.query_case(case_description, num_precedents)\n        \n        if \"error\" in result:\n            return f\"❌ {result['error']}\", \"\", \"\", \"\", \"\"\n        \n        # Format precedents\n        precedents_text = \"\"\n        for i, precedent in enumerate(result['relevant_precedents'], 1):\n            precedents_text += f\"**Precedent {i}** (Similarity: {precedent['similarity_score']:.3f})\\n\"\n            precedents_text += f\"{precedent['text'][:400]}...\\n\\n\"\n        \n        # Format analysis\n        analysis_text = result['legal_analysis']['analysis']\n        \n        # Format arguments\n        petitioner_text = result['petitioner_arguments']\n        respondent_text = result['respondent_arguments']\n        \n        return \"✅ Analysis completed successfully!\", precedents_text, analysis_text, petitioner_text, respondent_text\n        \n    except Exception as e:\n        return f\"❌ Error during analysis: {str(e)}\", \"\", \"\", \"\", \"\"\n\ndef search_precedents(query, num_results):\n    \"\"\"Search for relevant precedents\"\"\"\n    if not agent.knowledge_base_ready:\n        return \"❌ Knowledge base not ready. Please set up the knowledge base first.\"\n    \n    if not query.strip():\n        return \"❌ Please provide a search query.\"\n    \n    try:\n        results = agent.vector_store.search(query, k=num_results)\n        \n        search_results = f\"**Search Results for:** {query}\\n\\n\"\n        for i, result in enumerate(results, 1):\n            search_results += f\"**Result {i}** (Similarity: {result['similarity_score']:.3f})\\n\"\n            search_results += f\"{result['text'][:300]}...\\n\\n\"\n        \n        return search_results\n        \n    except Exception as e:\n        return f\"❌ Error during search: {str(e)}\"\n\n# Default case for demonstration\nDEFAULT_CASE = \"\"\"In 2025, a new case challenges a government policy that mandates all citizens to submit biometric data for accessing public services. A petitioner argues that the policy violates their privacy rights, citing the 2017 Supreme Court judgment that ruled the Right to Privacy as a fundamental right under Article 21 of the Indian Constitution.\n\nThe government argues that the biometric data collection is necessary for:\n1. Preventing fraud in public service delivery\n2. Ensuring efficient distribution of benefits\n3. National security considerations\n4. Digital India initiative compliance\n\nThe petitioner contends that:\n1. Mandatory biometric collection violates fundamental right to privacy\n2. No adequate safeguards for data protection\n3. Disproportionate state intrusion without compelling state interest\n4. Lack of informed consent and opt-out mechanisms\"\"\"\n\n# Create Gradio Interface\ndef create_gradio_interface():\n    \"\"\"Create the Gradio interface\"\"\"\n    \n    with gr.Blocks(title=\"Legal Intelligence Agentic System\", theme=gr.themes.Soft()) as interface:\n        \n        gr.HTML(\"\"\"\n        <div style=\"text-align: center; padding: 20px;\">\n            <h1>⚖️ Legal Intelligence Agentic System</h1>\n            <p>AI-powered legal case analysis with precedent-based reasoning</p>\n            <p><em>Hannoworks Internship Assignment Implementation</em></p>\n        </div>\n        \"\"\")\n        \n        with gr.Tab(\"🔧 Setup\"):\n            gr.Markdown(\"### Configure your Legal Intelligence System\")\n            \n            with gr.Row():\n                with gr.Column():\n                    api_key_input = gr.Textbox(\n                        label=\"Groq API Key\",\n                        placeholder=\"Enter your Groq API key here...\",\n                        type=\"password\",\n                        info=\"Get your free API key from https://console.groq.com/\"\n                    )\n                    \n                    legal_text_input = gr.Textbox(\n                        label=\"Legal Knowledge Base (Optional)\",\n                        placeholder=\"Paste your legal documents here or leave empty to use default...\",\n                        lines=10,\n                        info=\"Add your own legal texts or use the default privacy law precedents\"\n                    )\n                    \n                    setup_btn = gr.Button(\"🔨 Build Knowledge Base\", variant=\"primary\")\n                \n                with gr.Column():\n                    setup_status = gr.Textbox(label=\"Setup Status\", interactive=False)\n                    kb_info = gr.Textbox(label=\"Knowledge Base Info\", lines=5, interactive=False)\n            \n            setup_btn.click(\n                fn=setup_knowledge_base,\n                inputs=[api_key_input, legal_text_input],\n                outputs=[setup_status, kb_info]\n            )\n        \n        with gr.Tab(\"📋 Case Analysis\"):\n            gr.Markdown(\"### Analyze Legal Cases with AI-powered Reasoning\")\n            \n            with gr.Row():\n                with gr.Column(scale=2):\n                    case_input = gr.Textbox(\n                        label=\"Case Description\",\n                        placeholder=\"Describe the legal case you want to analyze...\",\n                        lines=8,\n                        value=DEFAULT_CASE,\n                        info=\"Provide detailed case facts, legal issues, and parties' positions\"\n                    )\n                    \n                    with gr.Row():\n                        precedents_slider = gr.Slider(\n                            minimum=3,\n                            maximum=10,\n                            value=5,\n                            step=1,\n                            label=\"Number of Precedents to Retrieve\"\n                        )\n                        \n                        analyze_btn = gr.Button(\"⚖️ Analyze Case\", variant=\"primary\")\n                \n                with gr.Column(scale=1):\n                    analysis_status = gr.Textbox(label=\"Analysis Status\", interactive=False)\n            \n            with gr.Row():\n                with gr.Column():\n                    precedents_output = gr.Textbox(\n                        label=\"📚 Relevant Precedents\",\n                        lines=10,\n                        interactive=False\n                    )\n                \n                with gr.Column():\n                    analysis_output = gr.Textbox(\n                        label=\"🔍 Legal Analysis\",\n                        lines=10,\n                        interactive=False\n                    )\n            \n            with gr.Row():\n                with gr.Column():\n                    petitioner_output = gr.Textbox(\n                        label=\"👤 Petitioner Arguments\",\n                        lines=8,\n                        interactive=False\n                    )\n                \n                with gr.Column():\n                    respondent_output = gr.Textbox(\n                        label=\"🏛️ Government/Respondent Arguments\",\n                        lines=8,\n                        interactive=False\n                    )\n            \n            analyze_btn.click(\n                fn=analyze_legal_case,\n                inputs=[case_input, precedents_slider],\n                outputs=[analysis_status, precedents_output, analysis_output, petitioner_output, respondent_output]\n            )\n        \n        with gr.Tab(\"🔍 Search Precedents\"):\n            gr.Markdown(\"### Search Legal Precedents and Case Law\")\n            \n            with gr.Row():\n                with gr.Column():\n                    search_query = gr.Textbox(\n                        label=\"Search Query\",\n                        placeholder=\"Enter legal concepts, case names, or constitutional provisions...\",\n                        info=\"e.g., 'right to privacy Article 21', 'biometric data collection', 'fundamental rights'\"\n                    )\n                    \n                    search_results_slider = gr.Slider(\n                        minimum=1,\n                        maximum=10,\n                        value=5,\n                        step=1,\n                        label=\"Number of Results\"\n                    )\n                    \n                    search_btn = gr.Button(\"🔍 Search\", variant=\"primary\")\n                \n                with gr.Column():\n                    search_output = gr.Textbox(\n                        label=\"Search Results\",\n                        lines=15,\n                        interactive=False\n                    )\n            \n            search_btn.click(\n                fn=search_precedents,\n                inputs=[search_query, search_results_slider],\n                outputs=[search_output]\n            )\n        \n        with gr.Tab(\"ℹ️ About\"):\n            gr.Markdown(\"\"\"\n            ## About Legal Intelligence Agentic System\n            \n            This system demonstrates an AI-powered legal intelligence platform that can:\n            \n            ### 🎯 Key Features:\n            - **Precedent-based Analysis**: Retrieves and analyzes relevant legal precedents\n            - **Multi-perspective Arguments**: Generates arguments for both petitioner and respondent\n            - **Constitutional Law Focus**: Specialized in Indian constitutional law and privacy rights\n            - **RAG Pipeline**: Uses Retrieval-Augmented Generation for accurate legal reasoning\n            - **Semantic Search**: Advanced vector similarity search for legal documents\n            \n            ### 🔧 Technical Architecture:\n            - **Document Processing**: Text chunking and cleaning\n            - **Vector Store**: FAISS-powered similarity search with Sentence Transformers\n            - **Legal Reasoning**: Groq LLM with specialized legal prompts\n            - **Agentic Workflow**: Multi-step reasoning pipeline\n            \n            ### 📚 Components:\n            1. **DocumentProcessor**: Handles legal text preprocessing\n            2. **VectorStore**: Manages embeddings and similarity search\n            3. **LegalReasoner**: Provides AI-powered legal analysis\n            4. **LegalAgent**: Orchestrates the complete workflow\n            \n            ### 🚀 Usage:\n            1. **Setup**: Enter your Groq API key and optionally add legal texts\n            2. **Analyze**: Input case descriptions for comprehensive analysis\n            3. **Search**: Find relevant precedents and case law\n            \n            ---\n            *Developed for Hannoworks Internship Assignment*\n            \"\"\")\n    \n    return interface\n\n# Launch the interface\nif __name__ == \"__main__\":\n    # Create and launch the interface\n    interface = create_gradio_interface()\n    \n    # For Kaggle, use share=True to create a public link\n    interface.launch(\n        share=True,  # Creates a public link for Kaggle\n        server_name=\"0.0.0.0\",  # Allows external access\n        server_port=7860,  # Default Gradio port\n        show_error=True,  # Show detailed errors\n        debug=True  # Enable debug mode\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T20:39:11.736353Z","iopub.execute_input":"2025-08-10T20:39:11.736671Z","iopub.status.idle":"2025-08-10T20:45:23.871213Z","shell.execute_reply.started":"2025-08-10T20:39:11.736638Z","shell.execute_reply":"2025-08-10T20:45:23.870582Z"}},"outputs":[{"name":"stderr","text":"2025-08-10 20:39:26.484689: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754858366.736151      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754858366.806024      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6bc5fdf74984e7dab0d23dbbbc4aab2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c202275a9e64b64ae75a442f82a74d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"459b68cc66be412dbaf2dee2e753607f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fc6c8a62ab74263bc14dd54ddd69071"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2c551b83bd84ae3a171545112221b09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"420d27442fba432ea3b0109cbc03241f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16c479e0676b4dbfbca013462f44f722"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f392a93f9b5470493d90e2cb37a54a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e90eb701bcc44e8f8f8515e2dc216713"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e95ef9f5f3c422b8cebd581e56ec271"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68fb2f3c61f245d5bde07c14b393e8d6"}},"metadata":{}},{"name":"stdout","text":"* Running on local URL:  http://0.0.0.0:7860\n* Running on public URL: https://c5bfc0c3cda5528dc6.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://c5bfc0c3cda5528dc6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98890302352044eab6243beb06526d6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e905a1191afd4a22a036aa573a605ba3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f852a4e313da4525b357fe00f1c909bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8ccc6cfcccd4736a09bc2368400fc82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c73aad5b819549a48f12fc94878f3b50"}},"metadata":{}},{"name":"stdout","text":"Keyboard interruption in main thread... closing server.\nKilling tunnel 0.0.0.0:7860 <> https://c5bfc0c3cda5528dc6.gradio.live\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}